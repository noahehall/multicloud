# s3

- scalable, distributed object storage accessible from anywhere

## my thoughts

## links

- [landing page](https://aws.amazon.com/s3/?did=ap_card&trk=ap_card)
- [bucket policy examples](https://docs.aws.amazon.com/en_us/AmazonS3/latest/userguide/example-bucket-policies.html)
- [https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html](https://aws.amazon.com/s3/storage-classes/)
- [versioning](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html)
- [naming rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html)
- [server side encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html)
- [storage class analysis](https://docs.aws.amazon.com/AmazonS3/latest/userguide/analytics-storage-class.html)

## best practices

- determine your folder hierarchy upfront: i.e your object prefixes
- s3 bucket policies vs iam policies
  - IAM policies: whenever you have alot of private buckets or prefer to centralize all of your policies
  - s3 bucket policies: simple cross-account access or complex policies that breach the IAM policy size limit
- object versioning
  - make sure to delete unused versions of an object to reduce costs
- automate object storage with lifecycle configurations
  - e.g. from standard > standard-IA > glacier: move to a lower cost longer term storage every 30 days
  - object types: periodic logs or any data that becomes less frequently accessed overtime
- static website hosting is a mismoner: you can still use client-side JS to load content dynamically
  - the idea is that there isnt a webserver responding to requests

### anti patterns

## features

- scalable storage with 11 9s of data durability
- multiple storage classes to fit R/W requirements
- various mechanisms for authnz; Configuring and enforcing data access controls
- REST APIs designed to work with any internet-development toolkit.
  - supports at least 3,500 requests per second to add data and 5,500 requests per second to retrieve data
- filter data retrieved by lambda with S3 Select (SQL)
- automatically Moving and storing data across different S3 storage classes
- parallel requests per object prefix
- use cases
  - big data analytics analytics
  - application data/log files/media hosting
  - backup and archival
  - software delivery: up to 5tb per object
  - static content/websites: html, css and clientside js
  - data lakes, cheap data store

### pricing

- depends on bucket versioning, object size, storage duration and storage class and the region of the s3 bucket
- storage class: think about storage duration and access patterns
  - latency of access: when you need something, do you need it immediately?
  - frequency of access: do you need stuff often
  - storage duration: long term vs short term
- costs are incurred for all objects in a bucket, including all object versions

## basics

- s3 is a flat, non-hierarchical structure where bucket names must be unique across all aws accounts and regions and bucket objects fake hierarchy via their name prefixes
- its all about buckets, and objects

### buckets

- All objects are stored in S3 buckets and can be organized with shared names called prefixes.
- bucket name: must be DNS compliant and unique across all of AWS, even tho the bucket itself is region specific
  - the bucket name becomes part of the URL for objects stored within

#### versioning

- enables uploading an item with the same prefix + object key
  - enabling versioning does not impact the location of the object
  - each successful upload creates an object with a new version ID
- object deletion:
  - deleting an object does not permanently remove it, instead a delete marker is placed on the object
  - to restore an object, you simply remove the marker
- version states
  - unversioned: object versioning is disabled
  - versioning-enabled: once enabled, it can never return to an unversioned state
  - versioning-suspended: all new objects will not have a version, but existing objects will keep their versions

#### folders

- are just the prefixes you give to objects in buckets

#### objects

- files stored in buckets
  - 5tb limit per object
- object key: the name of the object
- object url schema: `https://BUCKET_NAME.s3.REGION.amazonaws.com/PREFIX/OBJECT_KEY`
- parallel requests: scale per prefix
  - Write: 3.5k writes per second
  - Read: 5.5k reads per second
- strong read-after-write consistency for PUT and DELETE actions
- strongly consistent read operations on Amazon S3 Select, Amazon S3 access control lists, Amazon S3 object tags, and object metadata

#### batch operations

- streamlines data management at any scale, whether you store thousands of objects or a billion.
- when batch request completes you receive a notification and a completion report of all changes made.

### security

#### authnz

- all s3 resources are private by default: only the user/aws account can view resources
- bucket permissions: determines bucket + overall object permissions
  - object ownership: acls are disabled by default
- object permissions: for specific objects
- public access: 3 step process
  - unblock public access on bucket
  - enable ACLs for object ownership, confusing this is done at the bucket level as well
  - go to a specific object and make it public

##### s3 bucket policies

- resource policies

#### encryption

- provides encryption in transit and at rest
- at rest: three mutually exclusive options, depending on how you choose to manage the encryption keys.
  - Amazon S3-managed Keys (SSE-S3)
  - AWS KMS keys stored in AWS KMS (SSE-KMS)
  - Customer-provided keys (SSE-C)

### storage classes

- supports a specific data access level at corresponding costs or geographic location
- if you dont specify a storage class while uploading an object s3 uses the default (standard) class
- redundancy
  - one zone: the only storage class that doesnt store data redundantly across availability zones
  - all other classes store data across a minimum of 3 availability zones

#### standard (frequent access)

- general purpose for cloud apps, dynamic websites, content distribution, mobile/gaming apps, big data analytics

#### standard-infrequent access (standard IA)

- less frequent, highly available
- data that is access less frequently, but requires rapid access when needed
  - long term backups, disaster recovery files, etc
  - same durability, throughput and latency of s3 standard

#### one zone-infrequent access (one zone-IA)

- less frequent, less available
- stores data in a single availability zone
  - secondary backups or easily recreatable data

#### intelligent-tiering

- for data with unknown/changing access patterns
- s3 monitors object access patterns and moves them between 3 tiers
  - frequent access
  - infrequent access
  - archive instance

#### glacier storage classes (instant, flexible, archive)

- see [markdown file](./s3-glacier.md)
- for archiving data that is rarely access and requires millisecond retrieval
  - cost savings up 68% off standard-IA storage class
  - same latency and throughput performance of standard-IA

#### s3 on outposts

- see [markdown file](./s3-outposts.md)
- object storage for on-premise Outposts environment
- for workloads with local data residency requirements or onpremise performance reasons

#### lifecycle policies

- automate objects/groups of objects to appropriate storage tiers
- transition actions: define when objects should transition to another storage class
- expiration actions: define when objects expire and should be permanently deleted

##### Storage Class Analysis

- analyze storage access patterns to help you decide when to transition the right data to the right storage class
- use the report to configure an S3 Lifecycle policy that makes the data transfer.

## considerations

- region
- bucket name: 3-63 chars, DNS compatible
- object ownership: either public or define an ACL
- IAM policies vs s3 bucket policies
  - iam attached to users/groups/roles
  - bucket policies.....attached to buckets ;)
- bucket versioning
- storage lifecycle

## integrations
